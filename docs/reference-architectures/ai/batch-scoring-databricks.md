---
title: Assegnazione dei punteggi in batch per i modelli Spark in Azure Databricks
description: Creare una soluzione scalabile per l'assegnazione del punteggio batch a un modello di classificazione Apache Spark in una pianificazione usando Azure Databricks.
author: njray
ms.date: 02/07/2019
ms.topic: reference-architecture
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: azcat-ai
ms.openlocfilehash: 1b6f10edf098ed8d9fa14c16de113fc765372835
ms.sourcegitcommit: a68f248402c598f9d25bc1dc62f27a6a934ff001
ms.translationtype: HT
ms.contentlocale: it-IT
ms.lasthandoff: 02/08/2019
ms.locfileid: "55903273"
---
# <a name="batch-scoring-of-spark-models-on-azure-databricks"></a><span data-ttu-id="8d620-103">Assegnazione dei punteggi in batch per i modelli Spark in Azure Databricks</span><span class="sxs-lookup"><span data-stu-id="8d620-103">Batch scoring of Spark models on Azure Databricks</span></span>

<span data-ttu-id="8d620-104">Questa architettura di riferimento illustra come creare una soluzione scalabile per l'assegnazione del punteggio batch a un modello di classificazione Apache Spark in una pianificazione usando Azure Databricks, una piattaforma di strumenti analitici basata su Apache Spark ottimizzata per Azure.</span><span class="sxs-lookup"><span data-stu-id="8d620-104">This reference architecture shows how to build a scalable solution for batch scoring an Apache Spark classification model on a schedule using Azure Databricks, an Apache Spark-based analytics platform optimized for Azure.</span></span> <span data-ttu-id="8d620-105">La soluzione può essere usata come modello che può essere generalizzato per altri scenari.</span><span class="sxs-lookup"><span data-stu-id="8d620-105">The solution can be used as a template that can be generalized to other scenarios.</span></span>

<span data-ttu-id="8d620-106">Un'implementazione di riferimento per questa architettura è disponibile in  [GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="8d620-106">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Assegnazione dei punteggi in batch per i modelli Spark in Azure Databricks](./_images/batch-scoring-spark.png)

<span data-ttu-id="8d620-108">**Scenario**: un'azienda di un settore che fa largo uso di risorse vuole ridurre al minimo i costi e i tempi di inattività associati a guasti meccanici imprevisti.</span><span class="sxs-lookup"><span data-stu-id="8d620-108">**Scenario**: A business in an asset-heavy industry wants to minimize the costs and downtime associated with unexpected mechanical failures.</span></span> <span data-ttu-id="8d620-109">Usando i dati IoT raccolti dai computer aziendali, è possibile creare un modello di manutenzione predittiva.</span><span class="sxs-lookup"><span data-stu-id="8d620-109">Using IoT data collected from their machines, they can create a predictive maintenance model.</span></span> <span data-ttu-id="8d620-110">Questo modello consente all'azienda di gestire i componenti in maniera proattiva, riparandoli prima che si guastino.</span><span class="sxs-lookup"><span data-stu-id="8d620-110">This model enables the business to maintain components proactively and repair them before they fail.</span></span> <span data-ttu-id="8d620-111">Ottimizzando l'uso dei componenti meccanici, l'azienda può controllare i costi e ridurre i tempi di inattività.</span><span class="sxs-lookup"><span data-stu-id="8d620-111">By maximizing mechanical component use, they can control costs and reduce downtime.</span></span>

<span data-ttu-id="8d620-112">Un modello di manutenzione predittiva raccoglie i dati dai computer e conserva esempi cronologici dei guasti dei componenti.</span><span class="sxs-lookup"><span data-stu-id="8d620-112">A predictive maintenance model collects data from the machines and retains historical examples of component failures.</span></span> <span data-ttu-id="8d620-113">Il modello può quindi essere usato per monitorare lo stato corrente dei componenti e stimare se un determinato componente si guasterà nel prossimo futuro.</span><span class="sxs-lookup"><span data-stu-id="8d620-113">The model can then be used to monitor the current state of the components and predict if a given component will fail in the near future.</span></span> <span data-ttu-id="8d620-114">Per casi d'uso e metodi di modellazione comuni, vedere [Guida di Azure AI per soluzioni di manutenzione predittiva][ai-guide].</span><span class="sxs-lookup"><span data-stu-id="8d620-114">For common use cases and modeling approaches, see [Azure AI guide for predictive maintenance solutions][ai-guide].</span></span>

<span data-ttu-id="8d620-115">Questa architettura di riferimento è progettata per carichi di lavoro che vengono attivati dalla presenza di nuovi dati dai computer dei componenti.</span><span class="sxs-lookup"><span data-stu-id="8d620-115">This reference architecture is designed for workloads that are triggered by the presence of new data from the component machines.</span></span> <span data-ttu-id="8d620-116">L'elaborazione prevede i passaggi seguenti:</span><span class="sxs-lookup"><span data-stu-id="8d620-116">Processing involves the following steps:</span></span>

1. <span data-ttu-id="8d620-117">Inserire i dati dall'archivio dati esterno in un archivio dati di Azure Databricks.</span><span class="sxs-lookup"><span data-stu-id="8d620-117">Ingest the data from the external data store onto an Azure Databricks data store.</span></span>

2. <span data-ttu-id="8d620-118">Eseguire il training di un modello di Machine Learning trasformando i dati in un set di dati di training e quindi compilando un modello Spark MLlib.</span><span class="sxs-lookup"><span data-stu-id="8d620-118">Train a machine learning model by transforming the data into a training data set, then building a Spark MLlib model.</span></span> <span data-ttu-id="8d620-119">MLlib è costituito dagli algoritmi e le utilità di Machine Learning più comuni, ottimizzati per sfruttare i vantaggi delle funzionalità di scalabilità dei dati di Spark.</span><span class="sxs-lookup"><span data-stu-id="8d620-119">MLlib consists of most common machine learning algorithms and utilities optimized to take advantage of Spark data scalability capabilities.</span></span>

3. <span data-ttu-id="8d620-120">Applicare il modello sottoposto a training per stimare (classificare) i guasti dei componenti trasformando i dati in un set di dati di punteggio.</span><span class="sxs-lookup"><span data-stu-id="8d620-120">Apply the trained model to predict (classify) component failures by transforming the data into a scoring data set.</span></span> <span data-ttu-id="8d620-121">Assegnare un punteggio ai dati con il modello Spark MLlib.</span><span class="sxs-lookup"><span data-stu-id="8d620-121">Score the data with the Spark MLLib model.</span></span>

4. <span data-ttu-id="8d620-122">Archiviare i risultati nell'archivio dati di Databricks per l'utilizzo post-elaborazione.</span><span class="sxs-lookup"><span data-stu-id="8d620-122">Store results on the Databricks data store for post-processing consumption.</span></span>

<span data-ttu-id="8d620-123">Su  [GitHub][github] sono disponibili dei notebook per eseguire ognuna di queste attività.</span><span class="sxs-lookup"><span data-stu-id="8d620-123">Notebooks are provided on [GitHub][github] to perform each of these tasks.</span></span>

## <a name="architecture"></a><span data-ttu-id="8d620-124">Architettura</span><span class="sxs-lookup"><span data-stu-id="8d620-124">Architecture</span></span>

<span data-ttu-id="8d620-125">L'architettura definisce un flusso di dati interamente contenuto in [Azure Databricks][databricks] e basato su un set di [notebook][notebooks] eseguiti in sequenza.</span><span class="sxs-lookup"><span data-stu-id="8d620-125">The architecture defines a data flow that is entirely contained within [Azure Databricks][databricks] based on a set of sequentially executed [notebooks][notebooks].</span></span> <span data-ttu-id="8d620-126">È costituita dai componenti seguenti:</span><span class="sxs-lookup"><span data-stu-id="8d620-126">It consists of the following components:</span></span>

<span data-ttu-id="8d620-127">**[File di dati][github]**.</span><span class="sxs-lookup"><span data-stu-id="8d620-127">**[Data files][github]**.</span></span> <span data-ttu-id="8d620-128">L'implementazione di riferimento usa un set di dati simulato contenuto in cinque file di dati statici.</span><span class="sxs-lookup"><span data-stu-id="8d620-128">The reference implementation uses a simulated data set contained in five static data files.</span></span>

<span data-ttu-id="8d620-129">**[Inserimento][notebooks]**.</span><span class="sxs-lookup"><span data-stu-id="8d620-129">**[Ingestion][notebooks]**.</span></span> <span data-ttu-id="8d620-130">Il notebook di inserimento dati scarica i file di dati di input in una raccolta di set di dati di Databricks.</span><span class="sxs-lookup"><span data-stu-id="8d620-130">The data ingestion notebook downloads the input data files into a collection of Databricks data sets.</span></span> <span data-ttu-id="8d620-131">In uno scenario reale, i dati dei dispositivi IoT verrebbero trasmessi a un archivio accessibile da Databricks, come un archivio BLOB di Azure o Azure SQL Server.</span><span class="sxs-lookup"><span data-stu-id="8d620-131">In a real-world scenario, data from IoT devices would stream onto Databricks-accessible storage such as Azure SQL Server or Azure Blob storage.</span></span> <span data-ttu-id="8d620-132">Databricks supporta più [origini dati][data-sources].</span><span class="sxs-lookup"><span data-stu-id="8d620-132">Databricks supports multiple [data sources][data-sources].</span></span>

<span data-ttu-id="8d620-133">**Pipeline di training**.</span><span class="sxs-lookup"><span data-stu-id="8d620-133">**Training pipeline**.</span></span> <span data-ttu-id="8d620-134">Questo notebook esegue il notebook di progettazione delle funzionalità per creare un set di dati di analisi dai dati inseriti.</span><span class="sxs-lookup"><span data-stu-id="8d620-134">This notebook executes the feature engineering notebook to create an analysis data set from the ingested data.</span></span> <span data-ttu-id="8d620-135">Esegue quindi un notebook di compilazione modello che esegue il training del modello di Machine Learning usando la libreria di Machine Learning scalabile [Apache Spark MLlib][mllib].</span><span class="sxs-lookup"><span data-stu-id="8d620-135">It then executes a model building notebook that trains the machine learning model using the [Apache Spark MLlib][mllib] scalable machine learning library.</span></span>

<span data-ttu-id="8d620-136">**Pipeline di assegnazione del punteggio**.</span><span class="sxs-lookup"><span data-stu-id="8d620-136">**Scoring pipeline**.</span></span> <span data-ttu-id="8d620-137">Questo notebook esegue il notebook di progettazione delle funzionalità per creare un set di dati di punteggio dai dati inseriti, quindi esegue il notebook di assegnazione del punteggio.</span><span class="sxs-lookup"><span data-stu-id="8d620-137">This notebook executes the feature engineering notebook to create scoring data set from the ingested data and executes the scoring notebook.</span></span> <span data-ttu-id="8d620-138">Quest'ultimo usa il modello [Spark MLlib][mllib-spark] sottoposto a training per generare stime per le osservazioni nel set di dati di punteggio.</span><span class="sxs-lookup"><span data-stu-id="8d620-138">The scoring notebook uses the trained [Spark MLlib][mllib-spark] model to generate predictions for the observations in the scoring data set.</span></span> <span data-ttu-id="8d620-139">Le stime vengono archiviate nell'archivio risultati, un nuovo set di dati nell'archivio dati di Databricks.</span><span class="sxs-lookup"><span data-stu-id="8d620-139">The predictions are stored in the results store, a new data set on the Databricks data store.</span></span>

<span data-ttu-id="8d620-140">**Utilità di pianificazione**.</span><span class="sxs-lookup"><span data-stu-id="8d620-140">**Scheduler**.</span></span> <span data-ttu-id="8d620-141">Un [processo][job] pianificato di Databricks gestisce l'assegnazione del punteggio batch con il modello Spark.</span><span class="sxs-lookup"><span data-stu-id="8d620-141">A scheduled Databricks [job][job] handles batch scoring with the Spark model.</span></span> <span data-ttu-id="8d620-142">Il processo esegue il notebook della pipeline di assegnazione del punteggio, passando argomenti di variabile tramite i parametri del notebook per specificare i dettagli per la costruzione del set di dati di punteggio e la posizione in cui archiviare il set di dati dei risultati.</span><span class="sxs-lookup"><span data-stu-id="8d620-142">The job executes the scoring pipeline notebook, passing variable arguments through notebook parameters to specify the details for constructing the scoring data set and where to store the results data set.</span></span>

<span data-ttu-id="8d620-143">Lo scenario viene costruito come un flusso della pipeline.</span><span class="sxs-lookup"><span data-stu-id="8d620-143">The scenario is constructed as a pipeline flow.</span></span> <span data-ttu-id="8d620-144">Ogni notebook è ottimizzato per l'esecuzione in un'impostazione batch per ciascuna operazione: inserimento, progettazione delle funzionalità, compilazione del modello e assegnazione del punteggio al modello.</span><span class="sxs-lookup"><span data-stu-id="8d620-144">Each notebook is optimized to perform in a batch setting for each of the operations: ingestion, feature engineering, model building, and model scorings.</span></span> <span data-ttu-id="8d620-145">Per questo motivo il notebook di progettazione delle funzionalità è stato creato in modo da generare un set di dati generico per una qualsiasi delle operazioni di training, calibrazione, test o assegnazione del punteggio.</span><span class="sxs-lookup"><span data-stu-id="8d620-145">To accomplish this, the feature engineering notebook is designed to generate a general data set for any of the training, calibration, testing, or scoring operations.</span></span> <span data-ttu-id="8d620-146">In questo scenario viene usata una strategia di suddivisione temporale per queste operazioni, quindi i parametri del notebook vengono usati per impostare il filtro degli intervalli di date.</span><span class="sxs-lookup"><span data-stu-id="8d620-146">In this scenario, we use a temporal split strategy for these operations, so the notebook parameters are used to set date-range filtering.</span></span>

<span data-ttu-id="8d620-147">Poiché lo scenario crea una pipeline batch, viene fornito un set di notebook di esame facoltativi per esplorare l'output dei notebook di pipeline.</span><span class="sxs-lookup"><span data-stu-id="8d620-147">Because the scenario creates a batch pipeline, we provide a set of optional examination notebooks to explore the output of the pipeline notebooks.</span></span> <span data-ttu-id="8d620-148">Questi notebook sono disponibili nel repository GitHub:</span><span class="sxs-lookup"><span data-stu-id="8d620-148">You can find these in the GitHub repository:</span></span>

- `1a_raw-data_exploring`
- `2a_feature_exploration`
- `2b_model_testing`
- `3b_model_scoring_evaluation`

## <a name="recommendations"></a><span data-ttu-id="8d620-149">Consigli</span><span class="sxs-lookup"><span data-stu-id="8d620-149">Recommendations</span></span>

<span data-ttu-id="8d620-150">Databricks è configurato in modo da consentire di caricare e distribuire i modelli sottoposti a training per eseguire stime con i nuovi dati.</span><span class="sxs-lookup"><span data-stu-id="8d620-150">Databricks is set up so you can load and deploy your trained models to make predictions with new data.</span></span> <span data-ttu-id="8d620-151">Per questo scenario è stato usato Databricks per offrire i vantaggi aggiuntivi seguenti:</span><span class="sxs-lookup"><span data-stu-id="8d620-151">We used Databricks for this scenario because it provides these additional advantages:</span></span>

- <span data-ttu-id="8d620-152">Supporto del Single Sign-On mediante credenziali di Azure Active Directory.</span><span class="sxs-lookup"><span data-stu-id="8d620-152">Single sign-on support using Azure Active Directory credentials.</span></span>
- <span data-ttu-id="8d620-153">Utilità di pianificazione dei processi per eseguire i processi per le pipeline di produzione.</span><span class="sxs-lookup"><span data-stu-id="8d620-153">Job scheduler to execute jobs for production pipelines.</span></span>
- <span data-ttu-id="8d620-154">Notebook completamente interattivo con funzionalità di collaborazione, dashboard, API REST.</span><span class="sxs-lookup"><span data-stu-id="8d620-154">Fully interactive notebook with collaboration, dashboards, REST APIs.</span></span>
- <span data-ttu-id="8d620-155">Cluster illimitati scalabili a qualsiasi dimensione.</span><span class="sxs-lookup"><span data-stu-id="8d620-155">Unlimited clusters that can scale to any size.</span></span>
- <span data-ttu-id="8d620-156">Sicurezza avanzata, controllo degli accessi in base al ruolo e log di controllo.</span><span class="sxs-lookup"><span data-stu-id="8d620-156">Advanced security, role-based access controls, and audit logs.</span></span>

<span data-ttu-id="8d620-157">Per interagire con il servizio Azure Databricks, usare l'interfaccia [Workspace][workspace] di Databricks in un Web browser oppure l'[Interfaccia della riga di comando][cli].</span><span class="sxs-lookup"><span data-stu-id="8d620-157">To interact with the Azure Databricks service, use the Databricks [Workspace][workspace] interface in a web browser or the [command-line interface][cli] (CLI).</span></span> <span data-ttu-id="8d620-158">Accedere all'interfaccia della riga di comando di Databricks da qualsiasi piattaforma che supporti Python versioni da 2.7.9 a 3.6.</span><span class="sxs-lookup"><span data-stu-id="8d620-158">Access the Databricks CLI from any platform that supports Python 2.7.9 to 3.6.</span></span>

<span data-ttu-id="8d620-159">L'implementazione di riferimento usa i [notebook][notebooks] per eseguire le attività in sequenza.</span><span class="sxs-lookup"><span data-stu-id="8d620-159">The reference implementation uses [notebooks][notebooks] to execute tasks in sequence.</span></span> <span data-ttu-id="8d620-160">Ogni notebook archivia artefatti dei dati intermedi (set di dati di training, test, punteggio o risultati) nello stesso archivio dati dei dati di input.</span><span class="sxs-lookup"><span data-stu-id="8d620-160">Each notebook stores intermediate data artifacts (training, test, scoring, or results data sets) to the same data store as the input data.</span></span> <span data-ttu-id="8d620-161">L'obiettivo è semplificarne l'uso in base alle esigenze del proprio caso d'uso.</span><span class="sxs-lookup"><span data-stu-id="8d620-161">The goal is to make it easy for you to use it as needed in your particular use case.</span></span> <span data-ttu-id="8d620-162">In pratica, occorre connettere l'origine dati all'istanza di Azure Databricks per consentire ai notebook di leggere e scrivere direttamente nell'archivio.</span><span class="sxs-lookup"><span data-stu-id="8d620-162">In practice, you would connect your data source to your Azure Databricks instance for the notebooks to read and write directly back into your storage.</span></span>

<span data-ttu-id="8d620-163">È possibile monitorare l'esecuzione dei processi tramite l'interfaccia utente di Databricks, l'archivio dati o l'[interfaccia della riga di comando][cli] di Databricks, secondo le necessità.</span><span class="sxs-lookup"><span data-stu-id="8d620-163">You can monitor job execution through the Databricks user interface, the data store, or the Databricks [CLI][cli] as necessary.</span></span> <span data-ttu-id="8d620-164">Monitorare il cluster usando il [log eventi][log] e altre [metriche][metrics] fornite da Databricks.</span><span class="sxs-lookup"><span data-stu-id="8d620-164">Monitor the cluster using the [event log][log] and other [metrics][metrics] that Databricks provides.</span></span>

## <a name="performance-considerations"></a><span data-ttu-id="8d620-165">Considerazioni sulle prestazioni</span><span class="sxs-lookup"><span data-stu-id="8d620-165">Performance considerations</span></span>

<span data-ttu-id="8d620-166">Un cluster di Azure Databricks abilita la scalabilità automatica per impostazione predefinita, in modo che, in fase di runtime, Databricks possa riallocare dinamicamente i ruoli di lavoro all'account per le caratteristiche del processo.</span><span class="sxs-lookup"><span data-stu-id="8d620-166">An Azure Databricks cluster enables autoscaling by default so that during runtime, Databricks dynamically reallocates workers to account for the characteristics of your job.</span></span> <span data-ttu-id="8d620-167">Alcune parti della pipeline potrebbero richiedere più risorse di calcolo di altre.</span><span class="sxs-lookup"><span data-stu-id="8d620-167">Certain parts of your pipeline may be more computationally demanding than others.</span></span> <span data-ttu-id="8d620-168">Durante queste fasi del processo Databricks aggiunge altri ruoli di lavoro, rimuovendoli in seguito quando non sono più necessari.</span><span class="sxs-lookup"><span data-stu-id="8d620-168">Databricks adds additional workers during these phases of your job (and removes them when they’re no longer needed).</span></span> <span data-ttu-id="8d620-169">Con la scalabilità automatica è più facile ottenere un [utilizzo del cluster][cluster] elevato, perché non è necessario effettuare il provisioning del cluster per soddisfare un carico di lavoro.</span><span class="sxs-lookup"><span data-stu-id="8d620-169">Autoscaling makes it easier to achieve high [cluster utilization][cluster], because you don’t need to provision the cluster to match a workload.</span></span>

<span data-ttu-id="8d620-170">È inoltre possibile sviluppare pipeline pianificate più complesse usando [Azure Data Factory][adf] con Azure Databricks.</span><span class="sxs-lookup"><span data-stu-id="8d620-170">Additionally, more complex scheduled pipelines can be developed by using [Azure Data Factory][adf] with Azure Databricks.</span></span>

## <a name="storage-considerations"></a><span data-ttu-id="8d620-171">Considerazioni sulle risorse di archiviazione</span><span class="sxs-lookup"><span data-stu-id="8d620-171">Storage considerations</span></span>

<span data-ttu-id="8d620-172">In questa implementazione di riferimento i dati vengono archiviati direttamente nell'archivio di Databricks per semplicità.</span><span class="sxs-lookup"><span data-stu-id="8d620-172">In this reference implementation, the data is stored directly within Databricks storage for simplicity.</span></span> <span data-ttu-id="8d620-173">In un'impostazione di produzione, tuttavia, i dati possono essere archiviati in un archivio dati cloud come [Archivio BLOB di Azure][blob].</span><span class="sxs-lookup"><span data-stu-id="8d620-173">In a production setting, however, the data can be stored on cloud data storage such as [Azure Blob Storage][blob].</span></span> <span data-ttu-id="8d620-174">[Databricks][databricks-connect] supporta anche Azure Data Lake Store, Azure SQL Data Warehouse, Azure Cosmos DB, Apache Kafka e Hadoop.</span><span class="sxs-lookup"><span data-stu-id="8d620-174">[Databricks][databricks-connect] also supports Azure Data Lake Store, Azure SQL Data Warehouse, Azure Cosmos DB, Apache Kafka, and Hadoop.</span></span>

## <a name="cost-considerations"></a><span data-ttu-id="8d620-175">Considerazioni sul costo</span><span class="sxs-lookup"><span data-stu-id="8d620-175">Cost considerations</span></span>

<span data-ttu-id="8d620-176">Azure Databricks è un'offerta Spark premium a cui è associato un costo.</span><span class="sxs-lookup"><span data-stu-id="8d620-176">Azure Databricks is a premium Spark offering with an associated cost.</span></span> <span data-ttu-id="8d620-177">Sono inoltre disponibili [piani tariffari][pricing] standard e premium per Databricks.</span><span class="sxs-lookup"><span data-stu-id="8d620-177">In addition, there are standard and premium Databricks [pricing tiers][pricing].</span></span>

<span data-ttu-id="8d620-178">Per questo scenario è sufficiente il piano tariffario standard.</span><span class="sxs-lookup"><span data-stu-id="8d620-178">For this scenario, the standard pricing tier is sufficient.</span></span> <span data-ttu-id="8d620-179">Tuttavia, se un'applicazione specifica deve poter ridimensionare automaticamente i cluster per gestire carichi di lavoro di grandi dimensioni o dashboard interattivi di Databricks, il livello premium potrebbe aumentare ulteriormente i costi.</span><span class="sxs-lookup"><span data-stu-id="8d620-179">However, if your specific application requires automatically scaling clusters to handle larger workloads or interactive Databricks dashboards, the premium level could increase costs further.</span></span>

<span data-ttu-id="8d620-180">I notebook della soluzione possono essere eseguiti su qualsiasi piattaforma basata su Spark con modifiche minime per rimuovere i pacchetti specifici di Databricks.</span><span class="sxs-lookup"><span data-stu-id="8d620-180">The solution notebooks can run on any Spark-based platform with minimal edits to remove the Databricks-specific packages.</span></span> <span data-ttu-id="8d620-181">Vedere le soluzioni simili seguenti per diverse piattaforme di Azure:</span><span class="sxs-lookup"><span data-stu-id="8d620-181">See the following similar solutions for various Azure platforms:</span></span>

- <span data-ttu-id="8d620-182">[Python in Azure Machine Learning Studio][python-aml]</span><span class="sxs-lookup"><span data-stu-id="8d620-182">[Python on Azure Machine Learning Studio][python-aml]</span></span>
- <span data-ttu-id="8d620-183">[Servizi R per SQL Server][sql-r]</span><span class="sxs-lookup"><span data-stu-id="8d620-183">[SQL Server R services][sql-r]</span></span>
- <span data-ttu-id="8d620-184">[PySpark in Azure Data Science Virtual Machine][py-dvsm]</span><span class="sxs-lookup"><span data-stu-id="8d620-184">[PySpark on an Azure Data Science Virtual Machine][py-dvsm]</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="8d620-185">Distribuire la soluzione</span><span class="sxs-lookup"><span data-stu-id="8d620-185">Deploy the solution</span></span>

<span data-ttu-id="8d620-186">Per distribuire questa architettura di riferimento, seguire la procedura descritta nel repository  [GitHub][github] per compilare una soluzione scalabile per l'assegnazione di punteggi ai modelli Spark in batch in Azure Databricks.</span><span class="sxs-lookup"><span data-stu-id="8d620-186">To deploy this reference architecture, follow the steps described in the [GitHub][github] repository to build a scalable solution for scoring Spark models in batch on Azure Databricks.</span></span>

## <a name="related-architectures"></a><span data-ttu-id="8d620-187">Architetture correlate</span><span class="sxs-lookup"><span data-stu-id="8d620-187">Related architectures</span></span>

<span data-ttu-id="8d620-188">È disponibile anche un'architettura di riferimento che usa Spark per compilare [sistemi per raccomandazioni in tempo reale][recommendation] con punteggi offline precalcolati.</span><span class="sxs-lookup"><span data-stu-id="8d620-188">We have also built a reference architecture that uses Spark for building [real-time recommendation systems][recommendation] with offline, pre-computed scores.</span></span> <span data-ttu-id="8d620-189">Questi sistemi sono scenari comuni in cui i punteggi vengono elaborati in batch.</span><span class="sxs-lookup"><span data-stu-id="8d620-189">These recommendation systems are common scenarios where scores are batch-processed.</span></span>

[adf]: https://azure.microsoft.com/blog/operationalize-azure-databricks-notebooks-using-data-factory/
[ai-guide]: /azure/machine-learning/team-data-science-process/cortana-analytics-playbook-predictive-maintenance
[blob]: https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html
[cli]: https://docs.databricks.com/user-guide/dev-tools/databricks-cli.html
[cluster]: https://docs.azuredatabricks.net/user-guide/clusters/sizing.html
[databricks]: /azure/azure-databricks/
[databricks-connect]: /azure/azure-databricks/databricks-connect-to-data-sources
[data-sources]: https://docs.databricks.com/spark/latest/data-sources/index.html
[github]: https://github.com/Azure/BatchSparkScoringPredictiveMaintenance
[job]: https://docs.databricks.com/user-guide/jobs.html
[log]: https://docs.databricks.com/user-guide/clusters/event-log.html
[metrics]: https://docs.databricks.com/user-guide/clusters/metrics.html
[mllib]: https://docs.databricks.com/spark/latest/mllib/index.html
[mllib-spark]: https://docs.databricks.com/spark/latest/mllib/index.html#apache-spark-mllib
[notebooks]: https://docs.databricks.com/user-guide/notebooks/index.html
[pricing]: https://azure.microsoft.com/en-us/pricing/details/databricks/
[python-aml]: https://gallery.azure.ai/Notebook/Predictive-Maintenance-Modelling-Guide-Python-Notebook-1
[py-dvsm]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-using-PySpark
[recommendation]: /azure/architecture/reference-architectures/ai/real-time-recommendation
[sql-r]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-Modeling-Guide-using-SQL-R-Services-1
[workspace]: https://docs.databricks.com/user-guide/workspace.html
